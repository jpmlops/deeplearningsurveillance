{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Surveillance\n",
    "\n",
    "Surveillance systems, mainly composed of cameras, are today widespread in both indoor and outdoor environments.\n",
    "\n",
    "- Video surveillance involves the act of observing a scene or scenes and looking for specific behaviors that are improper or that may indicate the emergence or existence of improper behavior.\n",
    "\n",
    "- Common uses of video surveillance include observing the public at the entry to sports events, public transportation (train platforms, airports, etc.), and around the perimeter of secure facilities, especially those that are directly bounded by community spaces.\n",
    "\n",
    "In this project, convolutional neural networks using image recognition and their combination with recurrent neural networks temporal information is extracted using which an intelligent surveillance system is made.The LSTM Encoder-Decoder framework is used to learn representation of video sequences and applied for detect abnormal event in complex environment.The abnormal events are identified by computing the reconstruction loss using Euclidean distance between original and reconstructed batch.\n",
    "\n",
    "Our training dataset contains 16 training and 21 testing video clips. The videos are captured in CUHK campus avenue with 30652 (15328 training, 15324 testing) frames in total.\n",
    "\n",
    "This dataset accompanies paper \"Abnormal Event Detection at 150 FPS in Matlab, Cewu Lu, Jianping Shi, Jiaya Jia, International Conference on Computer Vision, (ICCV), 2013\".\n",
    "\n",
    "The training videos capture normal situations. Testing videos include both normal and abnormal events. Three abnormal activities as follows:\n",
    "\n",
    "- Strange action\t\n",
    "- Wrong direction\t\n",
    "- Abnormal object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 11:39:59.896652: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-25 11:39:59.952193: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-25 11:40:01.035228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os \n",
    "import cv2\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "from keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing current path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\Developer\\Desktop\\training\n",
    "os.chdir('/home/a2zproviders/Desktop/mlops/DeepSurveillance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/a2zproviders/Desktop/mlops/DeepSurveillance'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training data path and folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_image=[]\n",
    "# train_path='C:\\\\Users\\\\Developer\\\\Desktop\\\\training'\n",
    "train_path = \"/home/a2zproviders/Desktop/mlops/DeepSurveillance\"\n",
    "train_videos=os.listdir(train_path)\n",
    "train_images_path=train_path+'/frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "      \n",
    "    # creating a folder named data\n",
    "    if not os.path.exists('frames'):\n",
    "        os.makedirs('frames')\n",
    "  \n",
    "# if not created then raise error\n",
    "except OSError:\n",
    "    print ('Error: Creating directory of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for model\n",
    "\n",
    "- store_inarry function convert the images into array and store them as gray scale images array in store_image list. \n",
    "- into_frames function capture the video and read it frame by frame, it skips every 10 frames and capture a image from it because creating images for frame is computationally very high. \n",
    "- mean_squared_loss function is used for calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_inarray(image_path):\n",
    "    image=load_img(image_path)\n",
    "    image=img_to_array(image)\n",
    "    image=cv2.resize(image, (227,227), interpolation = cv2.INTER_AREA)\n",
    "    gray=0.2989*image[:,:,0]+0.5870*image[:,:,1]+0.1140*image[:,:,2]  ##To convert an RGB image to grayscale, you can use the RGB2GRAY command from the Image Processing Toolbox. If you do not have this toolbox, then you can use the standard NTSC conversion formula that is used for calculating the effective luminance of a pixel:intensity = 0.2989*red + 0.5870*green + 0.1140*blue\n",
    "    store_image.append(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_frames(train_path,video):\n",
    "    cam=cv2.VideoCapture(train_path+'\\\\'+video)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    # a variable to set how many frames you want to skip\n",
    "    frame_skip = 10\n",
    "    while cam.isOpened():\n",
    "        ret, frame = cam.read()     # read returns a tuple (bool value, image)\n",
    "        if not ret:\n",
    "            break\n",
    "        if i > frame_skip - 1:\n",
    "            # if video is still left continue creating images\n",
    "            name = './frames/' + video + '_image_' + str(j) + '.jpg'\n",
    "            print ('Creating...' + name)\n",
    "            cv2.imwrite(name, frame)\n",
    "            i = 0\n",
    "            continue\n",
    "        i += 1\n",
    "        j += 1\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(x1,x2):\n",
    "    difference=x1-x2\n",
    "    a,b,c,d,e=difference.shape\n",
    "    n_samples=a*b*c*d*e\n",
    "    sq_difference=difference**2\n",
    "    Sum=sq_difference.sum()\n",
    "    distance=np.sqrt(Sum)\n",
    "    mean_distance=distance/n_samples\n",
    "\n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating videos into images frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in train_videos:\n",
    "    into_frames(train_path,video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=os.listdir(train_images_path)\n",
    "for image in images:\n",
    "    image_path=train_images_path + '/' + image\n",
    "    store_inarray(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14990"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the standardized data in a numpy file\n",
    "\n",
    "We have standardized our data because it makes sure that data is internally consistent; that is, each data type has the same content and format. Standardized values are useful for tracking data that isn't easy to compare otherwise.Then we have finally stored our data in a numpy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m store_image\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(store_image,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m a,b,c\u001b[38;5;241m=\u001b[39mstore_image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m store_image\u001b[38;5;241m.\u001b[39mresize(b,c,a)\n\u001b[1;32m      5\u001b[0m store_image\u001b[38;5;241m=\u001b[39m(store_image\u001b[38;5;241m-\u001b[39mstore_image\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m/\u001b[39m(store_image\u001b[38;5;241m.\u001b[39mstd()) \u001b[38;5;66;03m# Normalization (x-mean)/standard_deviation (Data Standardization)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "store_image=np.array(store_image,dtype='uint8')\n",
    "a,b,c=store_image.shape\n",
    "\n",
    "store_image.resize(b,c,a)\n",
    "store_image=(store_image-store_image.mean())/(store_image.std()) # Normalization (x-mean)/standard_deviation (Data Standardization)\n",
    "store_image=np.clip(store_image,0,1)\n",
    "#print(store_image)\n",
    "np.save('training.npy',store_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv3D(filters=128,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',input_shape=(227,227,10,1),activation='relu'))\n",
    "model.add(Conv3D(filters=64,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='relu'))\n",
    "model.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True))\n",
    "model.add(ConvLSTM2D(filters=32,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True))\n",
    "model.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5))\n",
    "model.add(Conv3DTranspose(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='relu'))\n",
    "model.add(Conv3DTranspose(filters=1,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_data\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m frames\u001b[38;5;241m=\u001b[39mtraining_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      3\u001b[0m frames\u001b[38;5;241m=\u001b[39mframes\u001b[38;5;241m-\u001b[39mframes\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training.npy'"
     ]
    }
   ],
   "source": [
    "training_data=np.load('training.npy')\n",
    "frames=training_data.shape[2]\n",
    "frames=frames-frames%10\n",
    "\n",
    "training_data=training_data[:,:,:frames]\n",
    "training_data=training_data.reshape(-1,227,227,10)\n",
    "training_data=np.expand_dims(training_data,axis=4)\n",
    "target_data=training_data.copy()\n",
    "\n",
    "epochs=10\n",
    "batch_size=10\n",
    "\n",
    "callback_save = ModelCheckpoint(\"saved_model.h5\", monitor=\"mean_squared_error\", save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history=model.fit(training_data,target_data, batch_size=batch_size, epochs=epochs, callbacks = [callback_save,callback_early_stopping])\n",
    "model.save(\"saved_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of loss and accuracy on Training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m train_acc\u001b[38;5;241m=\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss=history.history['loss']\n",
    "train_acc=history.history['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_loss\u001b[49m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber_of_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('Number_of_epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train loss')\n",
    "plt.legend(['Train'])\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_acc)\n",
    "plt.xlabel('Number_of_epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train accuray')\n",
    "plt.legend(['Train'])\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 55, 55, 10, 128)   15616     \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 26, 26, 10, 64)    204864    \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 26, 26, 10, 64)    295168    \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 26, 26, 10, 32)    110720    \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 26, 26, 10, 64)    221440    \n",
      "_________________________________________________________________\n",
      "conv3d_transpose (Conv3DTran (None, 55, 55, 10, 128)   204928    \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTr (None, 227, 227, 10, 1)   15489     \n",
      "=================================================================\n",
      "Total params: 1,068,225\n",
      "Trainable params: 1,068,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\new_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"saved_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a test video file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"C:\\\\Users\\\\Developer\\\\Desktop\\\\Testing cam1\\\\cam12\\\\individual files\\\\ch03_20210719133111_normal.mp4\")\n",
    "print(cap.isOpened())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Abnormal activity in a test video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006873388674438122\n",
      "0.0006850778236904545\n",
      "0.0006832955674387294\n",
      "0.0006825481977133269\n",
      "0.000681964739440286\n",
      "0.0006813001804923627\n",
      "0.0006808422789048665\n",
      "0.0006807589807970203\n",
      "0.0006808015044200784\n",
      "0.0006809205364325589\n",
      "0.0006798685163908004\n",
      "0.0006810240595772615\n",
      "0.0006817710686419128\n",
      "0.0006830738244114242\n",
      "0.0006827911658895275\n",
      "0.0006833613671516413\n",
      "0.0006835183582997029\n",
      "0.0006837453529216484\n",
      "0.0006836973110223285\n",
      "0.0006840260050660809\n",
      "0.0006841596015733871\n",
      "0.000684113355618616\n",
      "0.0006845460981327504\n",
      "0.0006847436054186332\n",
      "0.0006846812246247346\n",
      "0.0006845697442177642\n",
      "0.0006841491846918875\n",
      "0.0006837660378708219\n",
      "0.0006835406019687371\n",
      "0.0006835635167627178\n",
      "0.0006837931139642006\n",
      "0.0006842298004747477\n",
      "0.0006898841877826472\n",
      "0.0006897003940438903\n",
      "0.0006900683200690102\n",
      "Abnormal Event Detected\n",
      "0.0006900699037743243\n",
      "Abnormal Event Detected\n",
      "0.0006901998599481975\n",
      "Abnormal Event Detected\n",
      "0.0006906202833737863\n",
      "Abnormal Event Detected\n",
      "0.0006909748613346756\n",
      "Abnormal Event Detected\n",
      "0.0006910113399569232\n",
      "Abnormal Event Detected\n",
      "0.0006913121226375361\n",
      "Abnormal Event Detected\n",
      "0.0006914012772111284\n",
      "Abnormal Event Detected\n",
      "0.0006918420341414683\n",
      "Abnormal Event Detected\n",
      "0.0006910377623485776\n",
      "Abnormal Event Detected\n",
      "0.0006910860103096029\n",
      "Abnormal Event Detected\n",
      "0.0006910879128144433\n",
      "Abnormal Event Detected\n",
      "0.0006915962746038296\n",
      "Abnormal Event Detected\n",
      "0.0006916702118913668\n",
      "Abnormal Event Detected\n",
      "0.0006921470087218702\n",
      "Abnormal Event Detected\n",
      "0.000692625878919867\n",
      "Abnormal Event Detected\n",
      "0.0006925981611911091\n"
     ]
    }
   ],
   "source": [
    "flag=0\n",
    "while cap.isOpened():\n",
    "    imagedump=[]\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    for i in range(10):\n",
    "        ret,frame=cap.read()\n",
    "        \n",
    "        if not hasattr(frame,'shape'):\n",
    "            flag=1\n",
    "            break\n",
    "        \n",
    "        image = imutils.resize(frame,width=640,height=360)\n",
    "\n",
    "        frame=cv2.resize(frame, (640,360), interpolation = cv2.INTER_AREA)\n",
    "        gray=0.2989*frame[:,:,0]+0.5870*frame[:,:,1]+0.1140*frame[:,:,2]\n",
    "        gray=(gray-gray.mean())/gray.std()\n",
    "        gray=np.clip(gray,0,1)\n",
    "        imagedump.append(gray)\n",
    "    \n",
    "    if flag==1:\n",
    "        break\n",
    "\n",
    "    imagedump=np.array(imagedump)\n",
    "    #print(imagedump)\n",
    "\n",
    "    imagedump.resize(227,227,10)\n",
    "    imagedump=np.expand_dims(imagedump,axis=0)\n",
    "    imagedump=np.expand_dims(imagedump,axis=4)\n",
    "    \n",
    "    #print(imagedump)\n",
    "\n",
    "    output=model.predict(imagedump)\n",
    "\n",
    "    loss=mean_squared_loss(imagedump,output)\n",
    "    \n",
    "    print(loss)\n",
    "\n",
    "    if frame.any()==None:\n",
    "        print(\"none\")\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "    if (loss>0.00066 and loss<0.000675) or (loss>0.00069 and loss<0.00071):                \n",
    "        print('Abnormal Event Detected')\n",
    "        cv2.putText(image,\"Abnormal Event\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),4,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"video\",image)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
